{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = torch.FloatTensor(input_size, output_size)\n",
    "        self.b = torch.FloatTensor(output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=torch.mm(x, self.W) + self.b\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x):\n",
    "    return 3 * x[:,0] + x[:,1] - 2 * x[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optim):\n",
    "    #initialize gradients in all parameters in module. \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    #feed-forward\n",
    "    y_hat = model(x)\n",
    "    #get error between answer and inferenced. \n",
    "    loss = ((y-y_hat)**2).sum() / x.size(0)\n",
    "    \n",
    "    #back-propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    #one-step of gradiant descent\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Natural language processing is one of the biggest streams in artificial intelligence, and it becomes very popular after seq2seq's invention.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Natural language processing is one of the biggest streams in artificial intelligence, and it becomes very popular after seq2seq's invention.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "class DataLoader(object):\n",
    "    \n",
    "    def __init__(self, train_fn, valid_fn, \n",
    "                batch_size=64, \n",
    "                device=-1, \n",
    "                max_vocab=999999, \n",
    "                min_freq=1, \n",
    "                use_eos=False, \n",
    "                shuffle=True\n",
    "                ):\n",
    "        super(DataLoader, self).__init__()\n",
    "        \n",
    "        # Define field of the input file. \n",
    "        # The input file consists of two fields. \n",
    "        self.label = data.Field(sequential=False, \n",
    "                               use_vocab=True, \n",
    "                               unk_token=None\n",
    "                               )\n",
    "        self.text=data.Field(use_vocab=True, \n",
    "                            batch_first=True, \n",
    "                            include_lengths=False, \n",
    "                            eos_token='<EOS>' if use_eos else None)\n",
    "        \n",
    "        # Those defined two columns will be delimited by TAB. \n",
    "        # Thus, we use TabularDataset to load two columns in the input file. \n",
    "        # We would have two separate input file: train_fn, valid_fn\n",
    "        # Files consist of two columns: label field and text field. \n",
    "        train, valid = data.TabularDataset.splits(path='', \n",
    "                                                 train=train_fn, \n",
    "                                                 validation=valid_fn, \n",
    "                                                 format='tsv', \n",
    "                                                 fields=[('label', self.label), \n",
    "                                                        ('text', self.text)\n",
    "                                                        ]\n",
    "                                                 )\n",
    "        \n",
    "        # Those loaded dataset would be feeded into each iterator:\n",
    "        # train iterator and valid iterator. \n",
    "        # We sort input sentences by length, to group similar lengths. \n",
    "        self.train_iter, self.valid_iter = data.BucketIterator.splits((train, valid), \n",
    "                                                                     batch_size=batch_size, \n",
    "                                                                     device='cuda:%d' % device if device >= 0 else 'cpu', \n",
    "                                                                     shuffle=shuffle, \n",
    "                                                                     sort_key=lambda x: len(x.text), \n",
    "                                                                     sort_within_batch=True\n",
    "                                                                     )\n",
    "        \n",
    "        # At last, we make a vocabulary for label and text field. \n",
    "        # It is making mapping table between wordsa nd indice. \n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "\n",
    "class Dataloader():\n",
    "    \n",
    "    def __init__(self, \n",
    "                train_fn, \n",
    "                valid_fn, \n",
    "                batch_size=64, \n",
    "                device='cpu', \n",
    "                max_vocab=99999999, \n",
    "                max_length=255, \n",
    "                fix_length=None, \n",
    "                use_bos=True, \n",
    "                use_eos=True, \n",
    "                shuffle=True\n",
    "                ):\n",
    "        \n",
    "        super(Dataloader, self).__init__()\n",
    "        \n",
    "        self.text = data.Field(sequential=True, \n",
    "                               device='cpu', \n",
    "                               max_vocab=99999999, \n",
    "                               max_length=255, \n",
    "                               fix_length=None, \n",
    "                               use_bos=True, \n",
    "                               use_eos=True, \n",
    "                               shuffle=True,\n",
    "                               dsl=False\n",
    "                              )\n",
    "        \n",
    "        super(Dataloader, self).__init__()\n",
    "        \n",
    "        self.src = data.Field(sequential=true, \n",
    "                              use_vocab=True, \n",
    "                              batch_first=True, \n",
    "                              include_lengths=True, \n",
    "                              fix_length=fix_length, \n",
    "                              init_token='<BOS>' if dsl else None, \n",
    "                              eos_token='<EOS>' if dsl else None\n",
    "                             )\n",
    "        \n",
    "        self.tgt = data.Field(sequential=true, \n",
    "                              use_vocab=True, \n",
    "                              batch_first=True, \n",
    "                              include_lengths=True, \n",
    "                              fix_length=fix_length, \n",
    "                              init_token='<BOS>' if use_bos else None, \n",
    "                              eos_token='<EOS>' if use_eos else None\n",
    "                             )\n",
    "        \n",
    "        if train_fn is not None and valid_fn is not None and exts is not none: \n",
    "            trainb = TranslationDataset(path=train_fn, \n",
    "                                       exts=exts, \n",
    "                                       fields=[('src', self.src), \n",
    "                                              ('tgt', self.tgt)\n",
    "                                              ], \n",
    "                                       max_length=max_length\n",
    "                                       )\n",
    "            valid = TranslationDataset(path=valid_fn, \n",
    "                                      exts=exts, \n",
    "                                      fields=[('src', self.src), \n",
    "                                             ('tgt', self.tgt)\n",
    "                                             ], \n",
    "                                      max_length=max_length\n",
    "                                      )\n",
    "            \n",
    "            self.train_iter = data.BucketIterator(train, \n",
    "                                                batch_size=batch_size, \n",
    "                                                device='cuda:%d' % device if device >= 0 else 'cpu', \n",
    "                                                shuffle=shuffle, \n",
    "                                                sort_key=lambda x: leng(x.tgt) + (max_length * len(x.src)), \n",
    "                                                sort_within_batch=true\n",
    "                                                )\n",
    "            \n",
    "            self.valid_iter = data.BucketIterator(valid, \n",
    "                                                 batch_size=batch_size, \n",
    "                                                 device='cuda:%d' % device if device >= 0 else 'cpu', \n",
    "                                                 shuffle=False, \n",
    "                                                 sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)), \n",
    "                                                 sort_within_batch=True\n",
    "                                                 )\n",
    "            \n",
    "            self.src.build_vocab(train, max_size=max_vocab)\n",
    "            self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "            \n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab\n",
    "        \n",
    "class TranslationDataset(data.Dataset):\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "    \n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "            \n",
    "        if not path.endswith('.'):\n",
    "            path +=  '.'\n",
    "        \n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "        \n",
    "        examples = []\n",
    "        with open(src_path, encoding ='utf-8') as src_file, open(trg_path, encoding='utf-8') as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                if max_length and max_length < max(len(src_line.split()), \n",
    "                                                  len(trg_line.split())\n",
    "                                                  ):\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples.append(data.Example.fromlist([src_line, trg_line], fields))\n",
    "                    \n",
    "        super().__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
